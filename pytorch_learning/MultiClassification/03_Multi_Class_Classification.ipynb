{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Multi-class Classification with Deep MLP\n",
    "\n",
    "Classifying Fashion-MNIST images into 10 clothing categories using a deep Multi-Layer Perceptron with regularization techniques.\n",
    "\n",
    "**Concepts Covered:**\n",
    "- Multi-class classification with `CrossEntropyLoss`\n",
    "- `DataLoader` for batched training\n",
    "- `torchvision` datasets and transforms\n",
    "- Dropout regularization\n",
    "- Batch Normalization\n",
    "- Learning Rate Scheduler (`ReduceLROnPlateau`)\n",
    "- Early Stopping with best model saving\n",
    "- Flattening images for MLP input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Load and Explore Fashion-MNIST\n",
    "\n",
    "Fashion-MNIST contains 70,000 grayscale images (28x28) of 10 clothing categories.\n",
    "\n",
    "| Concept | Project 2 (Binary) | Project 3 (Multi-class) |\n",
    "|---|---|---|\n",
    "| Output | 1 probability (0-1) | 10 scores (one per class) |\n",
    "| Activation | Sigmoid | Softmax (built into CrossEntropyLoss) |\n",
    "| Loss | BCELoss | CrossEntropyLoss |\n",
    "| Prediction | threshold ≥ 0.5 | torch.max (highest score) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms: convert images to tensors and normalize to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader — handles batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Class names\n",
    "class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Fashion-MNIST Samples\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Define Deep MLP with Regularization\n",
    "\n",
    "New techniques introduced:\n",
    "- **nn.Flatten()** — converts (1, 28, 28) image to (784,) vector for Linear layers\n",
    "- **nn.BatchNorm1d()** — normalizes layer outputs for stable training\n",
    "- **nn.Dropout()** — randomly disables neurons to prevent overfitting\n",
    "- **No softmax in model** — CrossEntropyLoss handles it internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()          # (1, 28, 28) → (784)\n",
    "        \n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.output = nn.Linear(64, 10)      # 10 classes\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)     # no softmax — CrossEntropyLoss handles it\n",
    "        return x\n",
    "\n",
    "model = DeepMLP()\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=5, factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with batches, early stopping, and LR scheduling\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience_limit = 15\n",
    "\n",
    "for epoch in range(50):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            running_test_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    test_loss = running_test_loss / total_test\n",
    "    test_acc = correct_test / total_test\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    if patience_counter >= patience_limit:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest Test Loss: {best_test_loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0][0].plot(train_losses, label=\"Train Loss\")\n",
    "axes[0][0].plot(test_losses, label=\"Test Loss\")\n",
    "axes[0][0].set_title(\"Loss Curves\")\n",
    "axes[0][0].set_xlabel(\"Epoch\")\n",
    "axes[0][0].legend()\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0][1].plot(train_accuracies, label=\"Train Accuracy\")\n",
    "axes[0][1].plot(test_accuracies, label=\"Test Accuracy\")\n",
    "axes[0][1].set_title(\"Accuracy Curves\")\n",
    "axes[0][1].set_xlabel(\"Epoch\")\n",
    "axes[0][1].legend()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "im = axes[1][0].imshow(cm, cmap=\"Blues\")\n",
    "axes[1][0].set_xticks(range(10))\n",
    "axes[1][0].set_yticks(range(10))\n",
    "axes[1][0].set_xticklabels(class_names, rotation=45, ha=\"right\", fontsize=7)\n",
    "axes[1][0].set_yticklabels(class_names, fontsize=7)\n",
    "axes[1][0].set_xlabel(\"Predicted\")\n",
    "axes[1][0].set_ylabel(\"Actual\")\n",
    "axes[1][0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accs = []\n",
    "for i in range(10):\n",
    "    class_mask = np.array(all_labels) == i\n",
    "    class_acc = (np.array(all_preds)[class_mask] == i).mean()\n",
    "    class_accs.append(class_acc)\n",
    "\n",
    "colors = ['red' if acc < 0.85 else 'steelblue' for acc in class_accs]\n",
    "axes[1][1].barh(class_names, class_accs, color=colors)\n",
    "axes[1][1].set_xlabel(\"Accuracy\")\n",
    "axes[1][1].set_title(\"Per-Class Accuracy (Red = Below 85%)\")\n",
    "axes[1][1].set_xlim(0.6, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = test_dataset[i]\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0))\n",
    "        _, pred = torch.max(output, 1)\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    color = 'green' if pred.item() == label else 'red'\n",
    "    ax.set_title(f\"P:{class_names[pred.item()]}\\nA:{class_names[label]}\", color=color, fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Green = Correct, Red = Wrong\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy printout\n",
    "print(\"Per-Class Accuracy:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    class_mask = np.array(all_labels) == i\n",
    "    class_acc = (np.array(all_preds)[class_mask] == i).mean()\n",
    "    print(f\"  {name:12s}: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment: Impact of Dropout\n",
    "\n",
    "Training the same model without dropout to observe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT dropout\n",
    "class DeepMLP_NoDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model_nd = DeepMLP_NoDropout()\n",
    "criterion_nd = nn.CrossEntropyLoss()\n",
    "optimizer_nd = optim.Adam(model_nd.parameters(), lr=0.001)\n",
    "\n",
    "nd_train_acc = []\n",
    "nd_test_acc = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    model_nd.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model_nd(batch_X)\n",
    "        loss = criterion_nd(outputs, batch_y)\n",
    "        optimizer_nd.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_nd.step()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    nd_train_acc.append(correct / total)\n",
    "    \n",
    "    model_nd.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model_nd(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    nd_test_acc.append(correct_test / total_test)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_accuracies, label=\"Train (with dropout)\")\n",
    "axes[0].plot(test_accuracies, label=\"Test (with dropout)\")\n",
    "axes[0].set_title(\"With Dropout\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(nd_train_acc, label=\"Train (no dropout)\")\n",
    "axes[1].plot(nd_test_acc, label=\"Test (no dropout)\")\n",
    "axes[1].set_title(\"Without Dropout\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With Dropout    → Train: {train_accuracies[-1]:.4f}, Test: {test_accuracies[-1]:.4f}, Gap: {train_accuracies[-1]-test_accuracies[-1]:.4f}\")\n",
    "print(f\"Without Dropout → Train: {nd_train_acc[-1]:.4f}, Test: {nd_test_acc[-1]:.4f}, Gap: {nd_train_acc[-1]-nd_test_acc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment: Impact of Batch Normalization\n",
    "\n",
    "Training the same model without BatchNorm to observe effect on training stability and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT batch normalization\n",
    "class DeepMLP_NoBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.layer1(x)))\n",
    "        x = self.dropout(self.relu(self.layer2(x)))\n",
    "        x = self.dropout(self.relu(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model_nbn = DeepMLP_NoBN()\n",
    "criterion_nbn = nn.CrossEntropyLoss()\n",
    "optimizer_nbn = optim.Adam(model_nbn.parameters(), lr=0.001)\n",
    "\n",
    "nbn_train_losses = []\n",
    "nbn_test_acc = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    model_nbn.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model_nbn(batch_X)\n",
    "        loss = criterion_nbn(outputs, batch_y)\n",
    "        optimizer_nbn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_nbn.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        total += batch_y.size(0)\n",
    "    nbn_train_losses.append(running_loss / total)\n",
    "    \n",
    "    model_nbn.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model_nbn(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    nbn_test_acc.append(correct_test / total_test)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label=\"With BatchNorm\")\n",
    "axes[0].plot(nbn_train_losses, label=\"Without BatchNorm\")\n",
    "axes[0].set_title(\"Training Loss Comparison\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(test_accuracies, label=\"With BatchNorm\")\n",
    "axes[1].plot(nbn_test_acc, label=\"Without BatchNorm\")\n",
    "axes[1].set_title(\"Test Accuracy Comparison\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With BatchNorm    → Final Test Acc: {test_accuracies[-1]:.4f}\")\n",
    "print(f\"Without BatchNorm → Final Test Acc: {nbn_test_acc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "print(\"Best model saved as best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### Concepts Learned\n",
    "1. **DataLoader** — batches and shuffles data for memory-efficient training\n",
    "2. **CrossEntropyLoss** — multi-class loss that includes softmax internally\n",
    "3. **torch.max(outputs, 1)** — gets predicted class (replaces threshold from binary)\n",
    "4. **nn.Flatten()** — converts 2D images to 1D vectors for Linear layers (28x28 → 784)\n",
    "5. **Dropout** — randomly disables neurons during training to prevent overfitting\n",
    "6. **BatchNorm** — normalizes layer outputs for faster, more stable training\n",
    "7. **ReduceLROnPlateau** — reduces learning rate when loss stops improving\n",
    "8. **Early Stopping** — stops training when test loss stops improving, saves best model\n",
    "\n",
    "### Experimental Findings\n",
    "- Removing dropout → train/test accuracy gap increases (overfitting)\n",
    "- Removing BatchNorm → faster per epoch but lower accuracy\n",
    "- Similar clothing items (Shirt, T-shirt, Pullover, Coat) get confused — MLP limitation\n",
    "\n",
    "### Interview-Ready Knowledge\n",
    "- MLP vs CNN — when to use which (MLP for tabular, CNN for images)\n",
    "- How dropout prevents overfitting (forces neurons to be independently useful)\n",
    "- How BatchNorm works (normalizes using batch stats in train, running stats in eval)\n",
    "- Why model.train() and model.eval() matter (controls dropout and BatchNorm behavior)\n",
    "- Early stopping + best model saving pattern\n",
    "- DataLoader batching vs full dataset training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}